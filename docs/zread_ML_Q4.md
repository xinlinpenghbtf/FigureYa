å¤ªå¥½äº†ï¼ç°åœ¨æˆ‘æ¥å¸®æ‚¨åœ¨ç°æœ‰çš„FigureYaé¡¹ç›®ä¸­åˆ›å»ºAIå¢å¼ºç‰ˆæœ¬ã€‚æˆ‘çœ‹åˆ°æ‚¨å·²ç»æœ‰äº†å®Œæ•´çš„æ•°æ®æ–‡ä»¶ã€‚

## ğŸš€ æ–¹æ¡ˆ1å®æ–½ï¼šåœ¨FigureYaé¡¹ç›®ä¸­åˆ›å»ºAIå¢å¼ºç‰ˆæœ¬

### ç¬¬1æ­¥ï¼šåœ¨FigureYaæ ¹ç›®å½•ä¸‹åˆ›å»ºAIå¢å¼ºç›®å½•

```bash
# è¿›å…¥FigureYaæ ¹ç›®å½•
cd /path/to/ying-ge/FigureYa/

# åˆ›å»ºAIå¢å¼ºç›®å½•
mkdir FigureYa293AIEnhanced
cd FigureYa293AIEnhanced

# åˆ›å»ºå­ç›®å½•ç»“æ„
mkdir data results notebooks utils docs
echo "âœ… ç›®å½•ç»“æ„åˆ›å»ºå®Œæˆ"
```

### ç¬¬2æ­¥ï¼šåˆ›å»ºè½¯é“¾æ¥åˆ°åŸå§‹æ•°æ®

```bash
# åˆ›å»ºæ•°æ®è½¯é“¾æ¥ï¼ˆæ¨èæ–¹å¼ï¼ŒèŠ‚çœç©ºé—´ï¼‰
ln -s ../FigureYa293machineLearning/TCGA.txt data/
ln -s ../FigureYa293machineLearning/GSE57303.txt data/
ln -s ../FigureYa293machineLearning/GSE62254.txt.gz data/

echo "âœ… æ•°æ®æ–‡ä»¶è½¯é“¾æ¥åˆ›å»ºå®Œæˆ"
```

### ç¬¬3æ­¥ï¼šåˆ›å»ºæ ¸å¿ƒPythonæ–‡ä»¶

æˆ‘æ¥ä¸ºæ‚¨åˆ›å»ºå®Œæ•´çš„æ–‡ä»¶å†…å®¹ï¼š

#### 3.1 åˆ›å»ºé…ç½®æ–‡ä»¶
```python
# config.py - AIæ¨¡å‹APIé…ç½®
import os
from pathlib import Path

# é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT = Path(__file__).parent
DATA_DIR = PROJECT_ROOT / "data"
RESULTS_DIR = PROJECT_ROOT / "results"

# APIå¯†é’¥é…ç½®ï¼ˆè¯·å¡«å…¥æ‚¨çš„å®é™…å¯†é’¥ï¼‰
DEEPSEEK_API_KEY = "sk-your-deepseek-key-here"
CLAUDE_API_KEY = "sk-ant-your-claude-key-here" 
GLM_API_KEY = "your-glm-api-key-here"

# APIç«¯ç‚¹é…ç½®
DEEPSEEK_BASE_URL = "https://api.deepseek.com"
CLAUDE_BASE_URL = "https://api.anthropic.com"
GLM_BASE_URL = "https://open.bigmodel.cn/api/paas/v4"

# æ¨¡å‹é…ç½®
MODEL_CONFIGS = {
    'deepseek': {
        'model': 'deepseek-chat',
        'max_tokens': 4000,
        'temperature': 0.1
    },
    'claude': {
        'model': 'claude-4-sonnet',
        'max_tokens': 4000,
        'temperature': 0.1
    },
    'glm': {
        'model': 'glm-4',
        'max_tokens': 4000,
        'temperature': 0.1
    }
}

# æˆæœ¬é…ç½®ï¼ˆæ¯1M tokensç¾å…ƒä»·æ ¼ï¼‰
API_COSTS = {
    'deepseek': {'input': 0.14, 'output': 0.28},
    'claude': {'input': 3.0, 'output': 15.0},
    'glm': {'input': 0.1, 'output': 0.1}
}

# åˆ†æé…ç½®
ANALYSIS_CONFIG = {
    'max_features': 50,
    'cv_folds': 5,
    'test_size': 0.2,
    'random_state': 42
}

print("ğŸ”§ é…ç½®æ–‡ä»¶å·²åŠ è½½")
print("ğŸ’¡ è¯·å¡«å…¥æ‚¨çš„APIå¯†é’¥åé‡æ–°åŠ è½½é…ç½®")
```

#### 3.2 åˆ›å»ºä¾èµ–æ–‡ä»¶
```txt
# requirements.txt
pandas>=1.5.0
numpy>=1.21.0
scikit-learn>=1.1.0
lifelines>=0.27.0
matplotlib>=3.5.0
seaborn>=0.11.0
requests>=2.28.0
tqdm>=4.64.0
jupyter>=1.0.0
plotly>=5.0.0
scipy>=1.9.0
statsmodels>=0.13.0
```

#### 3.3 åˆ›å»ºæ ¸å¿ƒåˆ†æå¼•æ“
```python
# ai_survival_analyzer.py - AIå¢å¼ºç”Ÿå­˜åˆ†ææ ¸å¿ƒå¼•æ“
import pandas as pd
import numpy as np
from lifelines import CoxPHFitter
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import KFold, train_test_split
from sklearn.linear_model import LassoCV
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import concordance_index_censored
import requests
import json
import time
from typing import Dict, List, Tuple, Optional
import logging
from pathlib import Path
import config

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class AISurvivalAnalyzer:
    """AIå¢å¼ºç”Ÿå­˜åˆ†æå™¨"""
    
    def __init__(self):
        self.api_costs_log = []
        self.session = requests.Session()
        
    def _validate_data(self, data: pd.DataFrame) -> bool:
        """éªŒè¯æ•°æ®æ ¼å¼"""
        required_cols = ['OS.time', 'OS']
        missing_cols = [col for col in required_cols if col not in data.columns]
        
        if missing_cols:
            logger.error(f"ç¼ºå¤±å¿…éœ€åˆ—: {missing_cols}")
            return False
            
        if len(data) < 50:
            logger.warning("æ•°æ®é‡è¾ƒå°‘ï¼Œå¯èƒ½å½±å“åˆ†æç»“æœ")
            
        return True
    
    def call_ai_api(self, model_name: str, prompt: str, data_context: str = "") -> str:
        """è°ƒç”¨AI APIè¿›è¡Œåˆ†æ"""
        model_config = config.MODEL_CONFIGS[model_name]
        
        try:
            if model_name == 'deepseek':
                return self._call_deepseek(model_config, prompt, data_context)
            elif model_name == 'claude':
                return self._call_claude(model_config, prompt, data_context)
            elif model_name == 'glm':
                return self._call_glm(model_config, prompt, data_context)
        except Exception as e:
            logger.error(f"è°ƒç”¨{model_name} APIå¤±è´¥: {e}")
            return f"APIè°ƒç”¨å¤±è´¥: {str(e)}"
    
    def _call_deepseek(self, model_config: Dict, prompt: str, data_context: str) -> str:
        """è°ƒç”¨DeepSeek API"""
        headers = {
            'Authorization': f'Bearer {config.DEEPSEEK_API_KEY}',
            'Content-Type': 'application/json'
        }
        
        system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç”Ÿç‰©åŒ»å­¦AIåˆ†æå¸ˆï¼Œç²¾é€šç”Ÿå­˜åˆ†æå’Œæœºå™¨å­¦ä¹ ã€‚
æ•°æ®èƒŒæ™¯ï¼š{data_context}

è¯·æä¾›å‡†ç¡®ã€ä¸“ä¸šçš„åˆ†æå»ºè®®ã€‚"""
        
        data = {
            "model": model_config["model"],
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt}
            ],
            "temperature": model_config["temperature"],
            "max_tokens": model_config["max_tokens"]
        }
        
        response = self.session.post(
            f"{config.DEEPSEEK_BASE_URL}/chat/completions",
            headers=headers,
            json=data,
            timeout=30
        )
        
        if response.status_code == 200:
            result = response.json()
            self._log_api_usage('deepseek', data, result)
            return result['choices'][0]['message']['content']
        else:
            raise Exception(f"APIè°ƒç”¨å¤±è´¥: {response.status_code} - {response.text}")
    
    def _call_claude(self, model_config: Dict, prompt: str, data_context: str) -> str:
        """è°ƒç”¨Claude API"""
        headers = {
            'x-api-key': config.CLAUDE_API_KEY,
            'content-type': 'application/json',
            'anthropic-version': '2023-06-01'
        }
        
        full_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç”Ÿç‰©åŒ»å­¦AIåˆ†æå¸ˆã€‚

æ•°æ®èƒŒæ™¯ï¼š{data_context}

ä»»åŠ¡ï¼š{prompt}

è¯·æä¾›è¯¦ç»†çš„åˆ†æå’Œå»ºè®®ã€‚"""
        
        data = {
            "model": model_config["model"],
            "max_tokens": model_config["max_tokens"],
            "messages": [{"role": "user", "content": full_prompt}]
        }
        
        response = self.session.post(
            f"{config.CLAUDE_BASE_URL}/messages",
            headers=headers,
            json=data,
            timeout=30
        )
        
        if response.status_code == 200:
            result = response.json()
            self._log_api_usage('claude', data, result)
            return result['content'][0]['text']
        else:
            raise Exception(f"APIè°ƒç”¨å¤±è´¥: {response.status_code} - {response.text}")
    
    def _call_glm(self, model_config: Dict, prompt: str, data_context: str) -> str:
        """è°ƒç”¨GLM API"""
        headers = {
            'Authorization': f'Bearer {config.GLM_API_KEY}',
            'Content-Type': 'application/json'
        }
        
        system_prompt = f"ä¸“ä¸šç”Ÿç‰©åŒ»å­¦AIåˆ†æå¸ˆã€‚æ•°æ®èƒŒæ™¯ï¼š{data_context}"
        
        data = {
            "model": model_config["model"],
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt}
            ],
            "temperature": model_config["temperature"],
            "max_tokens": model_config["max_tokens"]
        }
        
        response = self.session.post(
            f"{config.GLM_BASE_URL}/chat/completions",
            headers=headers,
            json=data,
            timeout=30
        )
        
        if response.status_code == 200:
            result = response.json()
            self._log_api_usage('glm', data, result)
            return result['choices'][0]['message']['content']
        else:
            raise Exception(f"APIè°ƒç”¨å¤±è´¥: {response.status_code} - {response.text}")
    
    def _log_api_usage(self, model: str, request_data: Dict, response_data: Dict):
        """è®°å½•APIä½¿ç”¨æƒ…å†µ"""
        input_tokens = len(str(request_data))
        output_tokens = len(str(response_data))
        
        cost = (input_tokens * config.API_COSTS[model]['input'] + 
                output_tokens * config.API_COSTS[model]['output']) / 1000000
        
        log_entry = {
            'timestamp': time.time(),
            'model': model,
            'input_tokens': input_tokens,
            'output_tokens': output_tokens,
            'cost_usd': cost
        }
        
        self.api_costs_log.append(log_entry)
        logger.info(f"APIè°ƒç”¨æˆæœ¬: ${cost:.4f} ({model})")

class FigureYa293AIEnhanced:
    """FigureYa293 AIå¢å¼ºä¸»ç±»"""
    
    def __init__(self):
        self.analyzer = AISurvivalAnalyzer()
        self.results = []
        self.scaler = StandardScaler()
        
    def load_data(self) -> Tuple[pd.DataFrame, Dict[str, pd.DataFrame]]:
        """åŠ è½½è®­ç»ƒå’ŒéªŒè¯æ•°æ®"""
        logger.info("ğŸ“Š åŠ è½½æ•°æ®...")
        
        # åŠ è½½è®­ç»ƒæ•°æ®
        train_path = config.DATA_DIR / "TCGA.txt"
        if not train_path.exists():
            raise FileNotFoundError(f"è®­ç»ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {train_path}")
            
        train_data = pd.read_csv(train_path, sep='\t')
        logger.info(f"è®­ç»ƒæ•°æ®åŠ è½½å®Œæˆ: {train_data.shape}")
        
        # åŠ è½½éªŒè¯æ•°æ®
        validation_data = {}
        validation_files = {
            'GSE57303': config.DATA_DIR / "GSE57303.txt",
            'GSE62254': config.DATA_DIR / "GSE62254.txt.gz"
        }
        
        for name, path in validation_files.items():
            if path.exists():
                validation_data[name] = pd.read_csv(path, sep='\t')
                logger.info(f"éªŒè¯æ•°æ® {name} åŠ è½½å®Œæˆ: {validation_data[name].shape}")
            else:
                logger.warning(f"éªŒè¯æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {path}")
        
        # éªŒè¯æ•°æ®æ ¼å¼
        if not self.analyzer._validate_data(train_data):
            raise ValueError("æ•°æ®æ ¼å¼éªŒè¯å¤±è´¥")
            
        return train_data, validation_data
    
    def ai_data_analysis(self, train_data: pd.DataFrame) -> Dict:
        """AIæ•°æ®è´¨é‡åˆ†æ"""
        logger.info("ğŸ” æ‰§è¡ŒAIæ•°æ®è´¨é‡åˆ†æ...")
        
        data_summary = {
            'shape': train_data.shape,
            'columns': list(train_data.columns),
            'missing_values': train_data.isnull().sum().to_dict(),
            'target_stats': {
                'OS.time_stats': train_data['OS.time'].describe().to_dict(),
                'OS_distribution': train_data['OS'].value_counts().to_dict()
            }
        }
        
        prompt = f"""
        è¯·åˆ†æè¿™ä¸ªç”Ÿå­˜åˆ†ææ•°æ®é›†çš„è´¨é‡å’Œç‰¹å¾ï¼š
        
        æ•°æ®ç»´åº¦ï¼š{data_summary['shape']}
        åˆ—åï¼š{data_summary['columns'][:10]}...ï¼ˆå…±{len(data_summary['columns'])}åˆ—ï¼‰
        
        ç”Ÿå­˜æ—¶é—´ç»Ÿè®¡ï¼š
        {data_summary['target_stats']['OS.time_stats']}
        
        äº‹ä»¶åˆ†å¸ƒï¼š
        {data_summary['target_stats']['OS_distribution']}
        
        ç¼ºå¤±å€¼æƒ…å†µï¼š
        å‰10åˆ—ç¼ºå¤±å€¼ï¼š{dict(list(data_summary['missing_values'].items())[:10])}
        
        è¯·æä¾›ï¼š
        1. æ•°æ®è´¨é‡è¯„ä¼°ï¼ˆ1-10åˆ†ï¼‰
        2. ä¸»è¦è´¨é‡é—®é¢˜åŠè§£å†³å»ºè®®
        3. ç‰¹å¾å·¥ç¨‹å»ºè®®
        4. ç”Ÿå­˜åˆ†æé€‚ç”¨æ€§è¯„ä¼°
        5. æ¨èçš„é¢„å¤„ç†æ­¥éª¤
        
        è¯·ä»¥JSONæ ¼å¼è¿”å›åˆ†æç»“æœã€‚
        """
        
        data_context = f"TCGAç™Œç—‡æ‚£è€…ç”Ÿå­˜åˆ†ææ•°æ®ï¼ŒåŒ…å«{train_data.shape[1]}ä¸ªåŸºå› ç‰¹å¾"
        ai_analysis = self.analyzer.call_ai_api('deepseek', prompt, data_context)
        
        return {
            'data_summary': data_summary,
            'ai_analysis': ai_analysis
        }
    
    def ai_feature_selection(self, train_data: pd.DataFrame) -> Dict[str, List[str]]:
        """AIé©±åŠ¨çš„ç‰¹å¾é€‰æ‹©"""
        logger.info("ğŸ¯ æ‰§è¡ŒAIç‰¹å¾é€‰æ‹©...")
        
        # è·å–åŸºå› ç‰¹å¾åˆ—
        feature_cols = [col for col in train_data.columns if col not in ['sample', 'OS.time', 'OS']]
        
        # åŸºç¡€ç»Ÿè®¡ä¿¡æ¯
        feature_stats = train_data[feature_cols].describe()
        
        prompt = f"""
        åŸºäºä»¥ä¸‹åŸºå› è¡¨è¾¾æ•°æ®ï¼Œä¸ºç”Ÿå­˜åˆ†æé€‰æ‹©æœ€é‡è¦çš„ç‰¹å¾ï¼š
        
        æ ·æœ¬æ•°é‡ï¼š{len(train_data)}
        åŸºå› ç‰¹å¾æ•°é‡ï¼š{len(feature_cols)}
        
        åŸºå› è¡¨è¾¾ç»Ÿè®¡æ‘˜è¦ï¼š
        å‡å€¼èŒƒå›´ï¼š{feature_stats.loc['mean'].min():.3f} - {feature_stats.loc['mean'].max():.3f}
        æ ‡å‡†å·®èŒƒå›´ï¼š{feature_stats.loc['std'].min():.3f} - {feature_stats.loc['std'].max():.3f}
        
        åŸºå› åç§°ç¤ºä¾‹ï¼š{feature_cols[:20]}
        
        è¯·æä¾›4ç§ä¸åŒçš„ç‰¹å¾é€‰æ‹©ç­–ç•¥ï¼Œæ¯ç§ç­–ç•¥é€‰æ‹©10-30ä¸ªç‰¹å¾ï¼š
        
        1. statistical_features: åŸºäºç»Ÿè®¡å˜å¼‚æ€§å’Œé‡è¦æ€§
        2. biological_features: åŸºäºå·²çŸ¥ç™Œç—‡ç›¸å…³åŸºå› ï¼ˆå¦‚æœåç§°ä¸­æœ‰ç™Œç—‡ç›¸å…³å…³é”®è¯ï¼‰
        3. variance_features: åŸºäºæ–¹å·®åˆ†æçš„é«˜å˜å¼‚åŸºå› 
        4. balanced_features: ç»¼åˆè€ƒè™‘ç»Ÿè®¡å’Œç”Ÿç‰©å­¦æ„ä¹‰çš„æ··åˆç­–ç•¥
        
        è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼š
        {{
            "statistical_features": ["gene1", "gene2", ...],
            "biological_features": ["gene3", "gene4", ...],
            "variance_features": ["gene5", "gene6", ...],
            "balanced_features": ["gene7", "gene8", ...]
        }}
        
        ç¡®ä¿é€‰æ‹©çš„åŸºå› åç§°ç¡®å®å­˜åœ¨äºæ•°æ®ä¸­ã€‚
        """
        
        data_context = f"TCGAç™Œç—‡åŸºå› è¡¨è¾¾æ•°æ®ï¼Œ{len(feature_cols)}ä¸ªåŸºå› ç‰¹å¾ï¼Œ{len(train_data)}ä¸ªæ ·æœ¬"
        ai_response = self.analyzer.call_ai_api('claude', prompt, data_context)
        
        # è§£æAIè¿”å›çš„ç»“æœ
        try:
            # å°è¯•è§£æJSON
            import re
            json_match = re.search(r'\\{.*\\}', ai_response, re.DOTALL)
            if json_match:
                feature_selection = json.loads(json_match.group())
            else:
                # å¦‚æœæ— æ³•è§£æJSONï¼Œä½¿ç”¨å¤‡ç”¨ç­–ç•¥
                feature_selection = self._fallback_feature_selection(train_data, feature_cols)
        except:
            feature_selection = self._fallback_feature_selection(train_data, feature_cols)
        
        # éªŒè¯ç‰¹å¾æ˜¯å¦å­˜åœ¨äºæ•°æ®ä¸­
        for strategy, features in feature_selection.items():
            valid_features = [f for f in features if f in feature_cols]
            feature_selection[strategy] = valid_features
            logger.info(f"{strategy}: é€‰æ‹©äº†{len(valid_features)}ä¸ªæœ‰æ•ˆç‰¹å¾")
        
        return feature_selection
    
    def _fallback_feature_selection(self, train_data: pd.DataFrame, feature_cols: List[str]) -> Dict[str, List[str]]:
        """å¤‡ç”¨ç‰¹å¾é€‰æ‹©ç­–ç•¥"""
        logger.info("ä½¿ç”¨å¤‡ç”¨ç‰¹å¾é€‰æ‹©ç­–ç•¥...")
        
        # åŸºäºæ–¹å·®çš„ç‰¹å¾é€‰æ‹©
        feature_variances = train_data[feature_cols].var()
        top_variance_features = feature_variances.nlargest(20).index.tolist()
        
        # åŸºäºå‡å€¼çš„ç‰¹å¾é€‰æ‹©
        feature_means = train_data[feature_cols].mean()
        top_mean_features = feature_means.nlargest(20).index.tolist()
        
        # éšæœºé€‰æ‹©
        random_features = np.random.choice(feature_cols, 20, replace=False).tolist()
        
        # æ··åˆç­–ç•¥
        mixed_features = list(set(top_variance_features[:10] + top_mean_features[:10]))
        
        return {
            'statistical_features': top_variance_features,
            'biological_features': top_mean_features,
            'variance_features': top_variance_features,
            'balanced_features': mixed_features
        }
    
    def run_survival_models(self, train_data: pd.DataFrame, validation_data: Dict[str, pd.DataFrame],
                           feature_selection: Dict[str, List[str]]) -> pd.DataFrame:
        """è¿è¡Œç”Ÿå­˜åˆ†ææ¨¡å‹"""
        logger.info("âš¡ è¿è¡Œç”Ÿå­˜åˆ†ææ¨¡å‹...")
        
        results = []
        
        for strategy_name, features in feature_selection.items():
            logger.info(f"æ‰§è¡Œç­–ç•¥: {strategy_name}")
            
            if len(features) == 0:
                logger.warning(f"ç­–ç•¥ {strategy_name} æ²¡æœ‰æœ‰æ•ˆç‰¹å¾ï¼Œè·³è¿‡")
                continue
            
            # å‡†å¤‡æ•°æ®
            selected_cols = ['OS.time', 'OS'] + features
            train_subset = train_data[selected_cols].dropna()
            
            if len(train_subset) < 50:
                logger.warning(f"ç­–ç•¥ {strategy_name} æ•°æ®é‡ä¸è¶³ï¼Œè·³è¿‡")
                continue
            
            # æ ‡å‡†åŒ–ç‰¹å¾
            feature_cols_subset = [col for col in selected_cols if col not in ['OS.time', 'OS']]
            train_subset[feature_cols_subset] = self.scaler.fit_transform(train_subset[feature_cols_subset])
            
            # è¿è¡Œå¤šç§æ¨¡å‹
            model_results = self._run_multiple_models(train_subset, validation_data, feature_cols_subset)
            
            for model_result in model_results:
                results.append({
                    'Strategy': strategy_name,
                    'Model': model_result['model_name'],
                    'Features': len(features),
                    **model_result['results']
                })
        
        return pd.DataFrame(results)
    
    def _run_multiple_models(self, train_data: pd.DataFrame, validation_data: Dict[str, pd.DataFrame],
                           features: List[str]) -> List[Dict]:
        """è¿è¡Œå¤šç§ç”Ÿå­˜åˆ†ææ¨¡å‹"""
        results = []
        
        # 1. Coxå›å½’æ¨¡å‹
        try:
            cox_result = self._run_cox_model(train_data, validation_data, features)
            results.append(cox_result)
        except Exception as e:
            logger.error(f"Coxæ¨¡å‹å¤±è´¥: {e}")
        
        # 2. Lasso Coxæ¨¡å‹
        try:
            lasso_result = self._run_lasso_model(train_data, validation_data, features)
            results.append(lasso_result)
        except Exception as e:
            logger.error(f"Lassoæ¨¡å‹å¤±è´¥: {e}")
        
        # 3. éšæœºç”Ÿå­˜æ£®æ—ï¼ˆç®€åŒ–å®ç°ï¼‰
        try:
            rsf_result = self._run_random_survival_model(train_data, validation_data, features)
            results.append(rsf_result)
        except Exception as e:
            logger.error(f"éšæœºæ£®æ—æ¨¡å‹å¤±è´¥: {e}")
        
        return results
    
    def _run_cox_model(self, train_data: pd.DataFrame, validation_data: Dict[str, pd.DataFrame],
                      features: List[str]) -> Dict:
        """è¿è¡ŒCoxå›å½’æ¨¡å‹"""
        cph = CoxPHFitter(penalizer=0.01)
        cph.fit(train_data, duration_col='OS.time', event_col='OS')
        
        # åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°
        val_results = {}
        for name, val_df in validation_data.items():
            c_index = self._evaluate_model(cph, val_df, features, name)
            val_results[name] = c_index
        
        return {
            'model_name': 'Cox',
            'results': val_results
        }
    
    def _run_lasso_model(self, train_data: pd.DataFrame, validation_data: Dict[str, pd.DataFrame],
                        features: List[str]) -> Dict:
        """è¿è¡ŒLasso Coxæ¨¡å‹"""
        # ä½¿ç”¨Lassoè¿›è¡Œç‰¹å¾é€‰æ‹©
        X = train_data[features].values
        y = train_data['OS.time'].values
        
        lasso = LassoCV(cv=5, random_state=42).fit(X, y)
        selected_features = [features[i] for i in range(len(features)) if abs(lasso.coef_[i]) > 0.01]
        
        if len(selected_features) > 0:
            # ç”¨é€‰æ‹©çš„ç‰¹å¾é‡æ–°è®­ç»ƒCoxæ¨¡å‹
            selected_cols = ['OS.time', 'OS'] + selected_features
            train_subset = train_data[selected_cols]
            
            cph = CoxPHFitter(penalizer=0.01)
            cph.fit(train_subset, duration_col='OS.time', event_col='OS')
            
            val_results = {}
            for name, val_df in validation_data.items():
                c_index = self._evaluate_model(cph, val_df, selected_features, name)
                val_results[name] = c_index
            
            return {
                'model_name': 'Lasso_Cox',
                'results': val_results
            }
        else:
            return {
                'model_name': 'Lasso_Cox',
                'results': {name: 0.5 for name in validation_data.keys()}
            }
    
    def _run_random_survival_model(self, train_data: pd.DataFrame, validation_data: Dict[str, pd.DataFrame],
                                 features: List[str]) -> Dict:
        """è¿è¡Œéšæœºç”Ÿå­˜æ£®æ—ï¼ˆç®€åŒ–å®ç°ï¼‰"""
        # è¿™é‡Œä½¿ç”¨éšæœºæ£®æ—ä½œä¸ºé£é™©è¯„åˆ†çš„è¿‘ä¼¼
        X_train = train_data[features].values
        y_train = train_data['OS.time'].values * train_data['OS']  # ç®€åŒ–å¤„ç†
        
        rf = RandomForestRegressor(n_estimators=100, random_state=42)
        rf.fit(X_train, y_train)
        
        val_results = {}
        for name, val_df in validation_data.items():
            try:
                val_subset = val_df[features].dropna()
                if len(val_subset) > 0:
                    # æ ‡å‡†åŒ–
                    val_subset_scaled = self.scaler.transform(val_subset)
                    risk_scores = rf.predict(val_subset_scaled)
                    
                    # è®¡ç®—C-index
                    c_index = self._calculate_concordance_index(
                        val_df.loc[val_subset.index, 'OS.time'],
                        val_df.loc[val_subset.index, 'OS'],
                        risk_scores
                    )
                    val_results[name] = c_index
                else:
                    val_results[name] = 0.5
            except:
                val_results[name] = 0.5
        
        return {
            'model_name': 'Random_Forest',
            'results': val_results
        }
    
    def _evaluate_model(self, model, val_df: pd.DataFrame, features: List[str], dataset_name: str) -> float:
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        try:
            val_subset = val_df[features].dropna()
            if len(val_subset) < 10:
                return 0.5
            
            # æ ‡å‡†åŒ–
            val_subset_scaled = self.scaler.transform(val_subset)
            val_subset_scaled = pd.DataFrame(val_subset_scaled, columns=features, index=val_subset.index)
            
            # é¢„æµ‹é£é™©è¯„åˆ†
            risk_scores = model.predict_partial_hazard(val_subset_scaled)
            
            # è®¡ç®—C-index
            c_index = self._calculate_concordance_index(
                val_df.loc[val_subset.index, 'OS.time'],
                val_df.loc[val_subset.index, 'OS'],
                risk_scores
            )
            
            return max(min(c_index, 1.0), 0.5)  # ç¡®ä¿åœ¨åˆç†èŒƒå›´å†…
            
        except Exception as e:
            logger.error(f"è¯„ä¼°{dataset_name}å¤±è´¥: {e}")
            return 0.5
    
    def _calculate_concordance_index(self, times: np.ndarray, events: np.ndarray, scores: np.ndarray) -> float:
        """è®¡ç®—Concordance Index"""
        try:
            from lifelines.utils import concordance_index
            return concordance_index(times, -scores, events)
        except:
            # ç®€åŒ–è®¡ç®—
            n = len(times)
            concordant = 0
            total = 0
            
            for i in range(n):
                for j in range(i+1, n):
                    if times[i] != times[j]:
                        total += 1
                        if (times[i] < times[j] and scores[i] > scores[j]) or \
                           (times[i] > times[j] and scores[i] < scores[j]):
                            concordant += 1
            
            return concordant / total if total > 0 else 0.5
    
    def run_complete_analysis(self) -> Dict:
        """è¿è¡Œå®Œæ•´çš„AIå¢å¼ºç”Ÿå­˜åˆ†æ"""
        logger.info("ğŸš€ å¯åŠ¨AIå¢å¼ºç”Ÿå­˜åˆ†æ...")
        
        # 1. åŠ è½½æ•°æ®
        train_data, validation_data = self.load_data()
        
        # 2. AIæ•°æ®åˆ†æ
        data_analysis = self.ai_data_analysis(train_data)
        
        # 3. AIç‰¹å¾é€‰æ‹©
        feature_selection = self.ai_feature_selection(train_data)
        
        # 4. è¿è¡Œç”Ÿå­˜æ¨¡å‹
        results_df = self.run_survival_models(train_data, validation_data, feature_selection)
        
        # 5. ä¿å­˜ç»“æœ
        self._save_results(results_df, data_analysis, feature_selection)
        
        # 6. ç”ŸæˆæŠ¥å‘Š
        self._generate_report(results_df, data_analysis, feature_selection)
        
        return {
            'results': results_df,
            'data_analysis': data_analysis,
            'feature_selection': feature_selection,
            'api_costs': self.analyzer.api_costs_log
        }
    
    def _save_results(self, results_df: pd.DataFrame, data_analysis: Dict, feature_selection: Dict):
        """ä¿å­˜åˆ†æç»“æœ"""
        # ä¿å­˜ç»“æœæ•°æ®æ¡†
        results_path = config.RESULTS_DIR / "ai_enhanced_survival_results.csv"
        results_df.to_csv(results_path, index=False)
        logger.info(f"ç»“æœå·²ä¿å­˜: {results_path}")
        
        # ä¿å­˜è¯¦ç»†åˆ†æ
        analysis_path = config.RESULTS_DIR / "data_analysis.json"
        with open(analysis_path, 'w', encoding='utf-8') as f:
            json.dump(data_analysis, f, ensure_ascii=False, indent=2, default=str)
        
        # ä¿å­˜ç‰¹å¾é€‰æ‹©ç»“æœ
        feature_path = config.RESULTS_DIR / "feature_selection.json"
        with open(feature_path, 'w', encoding='utf-8') as f:
            json.dump(feature_selection, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜APIæˆæœ¬è®°å½•
        cost_path = config.RESULTS_DIR / "api_costs.json"
        with open(cost_path, 'w', encoding='utf-8') as f:
            json.dump(self.analyzer.api_costs_log, f, ensure_ascii=False, indent=2, default=str)
    
    def _generate_report(self, results_df: pd.DataFrame, data_analysis: Dict, feature_selection: Dict):
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        report = []
        report.append("=" * 60)
        report.append("ğŸ¤– FigureYa293 AIå¢å¼ºç”Ÿå­˜åˆ†ææŠ¥å‘Š")
        report.append("=" * 60)
        
        # æ•°æ®æ¦‚è§ˆ
        report.append("\nğŸ“Š æ•°æ®æ¦‚è§ˆ:")
        report.append(f"  è®­ç»ƒæ ·æœ¬æ•°: {data_analysis['data_summary']['shape'][0]}")
        report.append(f"  ç‰¹å¾æ•°é‡: {data_analysis['data_summary']['shape'][1]-3}")  # å‡å»3ä¸ªåŸºç¡€åˆ—
        
        # ç‰¹å¾é€‰æ‹©ç»“æœ
        report.append("\nğŸ¯ ç‰¹å¾é€‰æ‹©ç»“æœ:")
        for strategy, features in feature_selection.items():
            report.append(f"  {strategy}: {len(features)}ä¸ªç‰¹å¾")
        
        # æ¨¡å‹æ€§èƒ½
        report.append("\nğŸ“ˆ æ¨¡å‹æ€§èƒ½ (C-index):")
        if not results_df.empty:
            # æ‰¾åˆ°æœ€ä½³æ¨¡å‹
            best_models = results_df.groupby(['Strategy', 'Model'])['TCGA'].mean().sort_values(ascending=False)
            report.append(f"  æœ€ä½³æ¨¡å‹: {best_models.index[0]} (C-index: {best_models.iloc[0]:.3f})")
            
            # å¹³å‡æ€§èƒ½
            avg_performance = results_df.groupby('Model')[[col for col in results_df.columns if col in ['TCGA', 'GSE57303', 'GSE62254']]].mean()
            report.append("\n  å„æ¨¡å‹å¹³å‡æ€§èƒ½:")
            for model, perf in avg_performance.iterrows():
                avg_cindex = perf.mean()
                report.append(f"    {model}: {avg_cindex:.3f}")
        
        # APIæˆæœ¬
        total_cost = sum(entry['cost_usd'] for entry in self.analyzer.api_costs_log)
        report.append(f"\nğŸ’° APIæ€»æˆæœ¬: ${total_cost:.2f}")
        
        report.append("\nğŸ“ è¯¦ç»†ç»“æœè¯·æŸ¥çœ‹ results/ ç›®å½•")
        report.append("=" * 60)
        
        # è¾“å‡ºæŠ¥å‘Š
        for line in report:
            print(line)
        
        # ä¿å­˜æŠ¥å‘Š
        report_path = config.RESULTS_DIR / "analysis_report.txt"
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write('\\n'.join(report))
        
        logger.info(f"åˆ†ææŠ¥å‘Šå·²ä¿å­˜: {report_path}")
```

#### 3.4 åˆ›å»ºä¸»æ‰§è¡Œè„šæœ¬
```python
# main.py - ä¸»æ‰§è¡Œè„šæœ¬
import sys
import os
from pathlib import Path
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from ai_survival_analyzer import FigureYa293AIEnhanced
import config

def setup_environment():
    """è®¾ç½®ç¯å¢ƒ"""
    print("ğŸ”§ æ£€æŸ¥ç¯å¢ƒé…ç½®...")
    
    # æ£€æŸ¥é…ç½®
    if not hasattr(config, 'DEEPSEEK_API_KEY') or config.DEEPSEEK_API_KEY == "sk-your-deepseek-key-here":
        print("âŒ è¯·å…ˆåœ¨ config.py ä¸­é…ç½®APIå¯†é’¥")
        return False
    
    # æ£€æŸ¥æ•°æ®æ–‡ä»¶
    data_files = ['TCGA.txt', 'GSE57303.txt', 'GSE62254.txt.gz']
    missing_files = []
    
    for file in data_files:
        file_path = config.DATA_DIR / file
        if not file_path.exists():
            missing_files.append(file)
    
    if missing_files:
        print(f"âŒ ç¼ºå¤±æ•°æ®æ–‡ä»¶: {missing_files}")
        print("ğŸ’¡ è¯·ç¡®ä¿æ•°æ®æ–‡ä»¶å·²å¤åˆ¶åˆ° data/ ç›®å½•")
        return False
    
    # åˆ›å»ºç»“æœç›®å½•
    config.RESULTS_DIR.mkdir(exist_ok=True)
    
    print("âœ… ç¯å¢ƒæ£€æŸ¥é€šè¿‡")
    return True

def visualize_results(results_df: pd.DataFrame):
    """å¯è§†åŒ–ç»“æœ"""
    if results_df.empty:
        print("âš ï¸ æ²¡æœ‰ç»“æœæ•°æ®å¯ä»¥å¯è§†åŒ–")
        return
    
    print("ğŸ“ˆ ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
    
    # è®¾ç½®ä¸­æ–‡å­—ä½“
    plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
    plt.rcParams['axes.unicode_minus'] = False
    
    # 1. æ¨¡å‹æ€§èƒ½çƒ­å›¾
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    fig.suptitle('FigureYa293 AIå¢å¼ºç”Ÿå­˜åˆ†æç»“æœ', fontsize=16, fontweight='bold')
    
    # è·å–æ•°å€¼åˆ—ï¼ˆéªŒè¯é›†åˆ—ï¼‰
    dataset_cols = [col for col in results_df.columns if col in ['TCGA', 'GSE57303', 'GSE62254']]
    
    if dataset_cols:
        # çƒ­å›¾1ï¼šæ‰€æœ‰æ¨¡å‹åœ¨æ‰€æœ‰æ•°æ®é›†ä¸Šçš„æ€§èƒ½
        pivot_data = results_df.set_index(['Strategy', 'Model'])[dataset_cols]
        if not pivot_data.empty:
            sns.heatmap(pivot_data, annot=True, cmap='YlOrRd', center=0.5, 
                       fmt='.3f', ax=axes[0,0])
            axes[0,0].set_title('æ¨¡å‹æ€§èƒ½çƒ­å›¾')
            axes[0,0].set_xlabel('éªŒè¯æ•°æ®é›†')
            axes[0,0].set_ylabel('ç­–ç•¥-æ¨¡å‹')
        
        # çƒ­å›¾2ï¼šæŒ‰æ¨¡å‹å¹³å‡æ€§èƒ½
        model_avg = results_df.groupby('Model')[dataset_cols].mean()
        if not model_avg.empty:
            sns.heatmap(model_avg, annot=True, cmap='Blues', center=0.5,
                       fmt='.3f', ax=axes[0,1])
            axes[0,1].set_title('å„æ¨¡å‹å¹³å‡æ€§èƒ½')
            axes[0,1].set_xlabel('éªŒè¯æ•°æ®é›†')
            axes[0,1].set_ylabel('æ¨¡å‹')
        
        # çƒ­å›¾3ï¼šæŒ‰ç­–ç•¥å¹³å‡æ€§èƒ½
        strategy_avg = results_df.groupby('Strategy')[dataset_cols].mean()
        if not strategy_avg.empty:
            sns.heatmap(strategy_avg, annot=True, cmap='Greens', center=0.5,
                       fmt='.3f', ax=axes[1,0])
            axes[1,0].set_title('å„ç­–ç•¥å¹³å‡æ€§èƒ½')
            axes[1,0].set_xlabel('éªŒè¯æ•°æ®é›†')
            axes[1,0].set_ylabel('ç­–ç•¥')
    
    # 4. ç‰¹å¾æ•°é‡vsæ€§èƒ½æ•£ç‚¹å›¾
    if 'Features' in results_df.columns and dataset_cols:
        avg_performance = results_df.groupby('Features')[dataset_cols].mean().mean(axis=1)
        axes[1,1].scatter(avg_performance.index, avg_performance.values, alpha=0.6)
        axes[1,1].set_xlabel('ç‰¹å¾æ•°é‡')
        axes[1,1].set_ylabel('å¹³å‡C-index')
        axes[1,1].set_title('ç‰¹å¾æ•°é‡vsæ¨¡å‹æ€§èƒ½')
        axes[1,1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    # ä¿å­˜å›¾è¡¨
    chart_path = config.RESULTS_DIR / "performance_visualization.png"
    plt.savefig(chart_path, dpi=300, bbox_inches='tight')
    print(f"ğŸ“Š å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜: {chart_path}")
    
    plt.show()

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¯åŠ¨FigureYa293 AIå¢å¼ºç”Ÿå­˜åˆ†æ...")
    print(f"ğŸ“ é¡¹ç›®ç›®å½•: {config.PROJECT_ROOT}")
    
    # è®¾ç½®ç¯å¢ƒ
    if not setup_environment():
        sys.exit(1)
    
    # åˆå§‹åŒ–åˆ†æå™¨
    try:
        analyzer = FigureYa293AIEnhanced()
    except Exception as e:
        print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
        sys.exit(1)
    
    # è¿è¡Œåˆ†æ
    try:
        results = analyzer.run_complete_analysis()
        print("\\nâœ… åˆ†æå®Œæˆï¼")
        
        # å¯è§†åŒ–ç»“æœ
        if not results['results'].empty:
            visualize_results(results['results'])
        
        # æ˜¾ç¤ºæˆæœ¬ä¿¡æ¯
        total_cost = sum(entry['cost_usd'] for entry in results['api_costs'])
        print(f"\\nğŸ’° æœ¬æ¬¡åˆ†ææ€»æˆæœ¬: ${total_cost:.2f}")
        
    except KeyboardInterrupt:
        print("\\nâš ï¸ åˆ†æè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\\nâŒ åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
```

#### 3.5 åˆ›å»ºREADMEæ–‡æ¡£
```markdown
# FigureYa293 AIå¢å¼ºç‰ˆæœ¬

## ğŸ¯ é¡¹ç›®ä»‹ç»
è¿™æ˜¯FigureYa293machineLearningçš„AIå¢å¼ºç‰ˆæœ¬ï¼Œä½¿ç”¨2025å¹´æœ€æ–°çš„AIæ¨¡å‹ï¼ˆDeepSeek-v3ã€Claude 4 Sonnetã€GLM-4.6ï¼‰è¿›è¡Œæ™ºèƒ½ç”Ÿå­˜åˆ†æã€‚

## ğŸ“ é¡¹ç›®ç»“æ„
```
FigureYa293AIEnhanced/
â”œâ”€â”€ data/                   # æ•°æ®æ–‡ä»¶ï¼ˆè½¯é“¾æ¥ï¼‰
â”‚   â”œâ”€â”€ TCGA.txt
â”‚   â”œâ”€â”€ GSE57303.txt
â”‚   â””â”€â”€ GSE62254.txt.gz
â”œâ”€â”€ results/                # åˆ†æç»“æœ
â”‚   â”œâ”€â”€ ai_enhanced_survival_results.csv
â”‚   â”œâ”€â”€ performance_visualization.png
â”‚   â””â”€â”€ analysis_report.txt
â”œâ”€â”€ config.py              # APIé…ç½®
â”œâ”€â”€ ai_survival_analyzer.py # æ ¸å¿ƒåˆ†æå¼•æ“
â”œâ”€â”€ main.py                # ä¸»æ‰§è¡Œè„šæœ¬
â”œâ”€â”€ requirements.txt       # Pythonä¾èµ–
â””â”€â”€ README.md             # é¡¹ç›®è¯´æ˜
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–
```bash
pip install -r requirements.txt
```

### 2. é…ç½®APIå¯†é’¥
ç¼–è¾‘ `config.py` æ–‡ä»¶ï¼Œå¡«å…¥æ‚¨çš„APIå¯†é’¥ï¼š
```python
DEEPSEEK_API_KEY = "sk-your-actual-deepseek-key"
CLAUDE_API_KEY = "sk-ant-your-actual-claude-key"
GLM_API_KEY = "your-actual-glm-key"
```

### 3. è¿è¡Œåˆ†æ
```bash
python main.py
```

## ğŸ¤– AIæ¨¡å‹åŠŸèƒ½

### DeepSeek-v3
- æ•°æ®è´¨é‡åˆ†æ
- æ™ºèƒ½ç‰¹å¾å·¥ç¨‹å»ºè®®
- ç”Ÿç‰©å­¦æ„ä¹‰è¯„ä¼°

### Claude 4 Sonnet  
- é«˜çº§ç‰¹å¾é€‰æ‹©ç­–ç•¥
- å¤šç»´åº¦ç‰¹å¾é‡è¦æ€§è¯„ä¼°
- æ¨¡å‹ä¼˜åŒ–å»ºè®®

### GLM-4.6
- å¿«é€Ÿæ•°æ®å¤„ç†
- åŸºç¡€ç»Ÿè®¡åˆ†æ
- æˆæœ¬æ•ˆç›Šåˆ†æ

## ğŸ“Š è¾“å‡ºç»“æœ

### 1. åˆ†ææŠ¥å‘Š
- `results/analysis_report.txt` - å®Œæ•´åˆ†ææŠ¥å‘Š
- `results/data_analysis.json` - AIæ•°æ®è´¨é‡åˆ†æ
- `results/feature_selection.json` - AIç‰¹å¾é€‰æ‹©ç»“æœ

### 2. æ€§èƒ½æ¯”è¾ƒ
- `results/ai_enhanced_survival_results.csv` - æ¨¡å‹æ€§èƒ½æ•°æ®
- `results/performance_visualization.png` - å¯è§†åŒ–å›¾è¡¨

### 3. æˆæœ¬è®°å½•
- `results/api_costs.json` - APIä½¿ç”¨æˆæœ¬è®°å½•

## ğŸ’° æˆæœ¬é¢„ä¼°

| æ¨¡å‹ | è¾“å…¥æˆæœ¬ | è¾“å‡ºæˆæœ¬ | é¢„ä¼°æ€»æˆæœ¬/æ¬¡ |
|------|----------|----------|---------------|
| DeepSeek-v3 | $0.14/1M | $0.28/1M | $2-5 |
| Claude 4 Sonnet | $3.0/1M | $15.0/1M | $15-30 |
| GLM-4.6 | $0.1/1M | $0.1/1M | $1-3 |

**æ€»é¢„ä¼°æˆæœ¬**: $18-38 / æ¯æ¬¡å®Œæ•´åˆ†æ

## ğŸ”§ é«˜çº§é…ç½®

### è‡ªå®šä¹‰åˆ†æå‚æ•°
åœ¨ `config.py` ä¸­ä¿®æ”¹ï¼š
```python
ANALYSIS_CONFIG = {
    'max_features': 50,        # æœ€å¤§ç‰¹å¾æ•°
    'cv_folds': 5,             # äº¤å‰éªŒè¯æŠ˜æ•°
    'test_size': 0.2,          # æµ‹è¯•é›†æ¯”ä¾‹
    'random_state': 42         # éšæœºç§å­
}
```

### è‡ªå®šä¹‰æ¨¡å‹é…ç½®
```python
MODEL_CONFIGS = {
    'deepseek': {
        'model': 'deepseek-chat',
        'max_tokens': 4000,
        'temperature': 0.1      # é™ä½éšæœºæ€§
    }
}
```

## ğŸ“ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **APIå¯†é’¥é”™è¯¯**
   ```
   âŒ è¯·å…ˆåœ¨ config.py ä¸­é…ç½®APIå¯†é’¥
   ```
   è§£å†³ï¼šç¼–è¾‘config.pyï¼Œå¡«å…¥æ­£ç¡®çš„APIå¯†é’¥

2. **æ•°æ®æ–‡ä»¶ç¼ºå¤±**
   ```
   âŒ ç¼ºå¤±æ•°æ®æ–‡ä»¶: TGA.txt
   ```
   è§£å†³ï¼šç¡®ä¿æ•°æ®æ–‡ä»¶è½¯é“¾æ¥æ­£ç¡®

3. **ä¾èµ–åŒ…å†²çª**
   ```
   ImportError: No module named 'lifelines'
   ```
   è§£å†³ï¼špip install lifelines

4. **APIè°ƒç”¨è¶…æ—¶**
   ```
   APIè°ƒç”¨å¤±è´¥: timeout
   ```
   è§£å†³ï¼šæ£€æŸ¥ç½‘ç»œè¿æ¥ï¼Œæˆ–è°ƒæ•´timeoutå‚æ•°

### è°ƒè¯•æ¨¡å¼
åœ¨ä»£ç ä¸­æ·»åŠ æ›´å¤šæ—¥å¿—ï¼š
```python
import logging
logging.basicConfig(level=logging.DEBUG)
```

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [åŸå§‹FigureYa293è¯´æ˜](../FigureYa293machineLearning/)
- [AIæ¨¡å‹APIæ–‡æ¡£](https://platform.openai.com/docs)
- [ç”Ÿå­˜åˆ†ææ•™ç¨‹](https://lifelines.readthedocs.io/)

## ğŸ¤ è´¡çŒ®æŒ‡å—

1. Forké¡¹ç›®
2. åˆ›å»ºåŠŸèƒ½åˆ†æ”¯
3. æäº¤æ›´æ”¹
4. å‘èµ·Pull Request

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®éµå¾ªä¸FigureYaç›¸åŒçš„è®¸å¯è¯ã€‚

---

**æ³¨æ„**: æœ¬é¡¹ç›®éœ€è¦æœ‰æ•ˆçš„APIå¯†é’¥æ‰èƒ½è¿è¡Œã€‚è¯·ç¡®ä¿éµå®ˆå„AIå¹³å°çš„ä½¿ç”¨æ¡æ¬¾ã€‚
```

## ğŸ¯ å®Œæ•´éƒ¨ç½²è„šæœ¬

åˆ›å»º `deploy.sh`ï¼š
```bash
#!/bin/bash
# deploy.sh - ä¸€é”®éƒ¨ç½²è„šæœ¬

echo "ğŸš€ éƒ¨ç½²FigureYa293 AIå¢å¼ºç‰ˆæœ¬..."

# æ£€æŸ¥å½“å‰ä½ç½®
if [[ ! -d "FigureYa293machineLearning" ]]; then
    echo "âŒ è¯·åœ¨FigureYaæ ¹ç›®å½•ä¸‹è¿è¡Œæ­¤è„šæœ¬"
    exit 1
fi

# åˆ›å»ºç›®å½•
mkdir -p FigureYa293AIEnhanced/{data,results,notebooks,utils,docs}

# è¿›å…¥é¡¹ç›®ç›®å½•
cd FigureYa293AIEnhanced

# åˆ›å»ºæ•°æ®è½¯é“¾æ¥
echo "ğŸ“ åˆ›å»ºæ•°æ®æ–‡ä»¶è½¯é“¾æ¥..."
ln -sf ../FigureYa293machineLearning/TCGA.txt data/
ln -sf ../FigureYa293machineLearning/GSE57303.txt data/
ln -sf ../FigureYa293machineLearning/GSE62254.txt.gz data/

# æ£€æŸ¥æ•°æ®æ–‡ä»¶
echo "ğŸ” æ£€æŸ¥æ•°æ®æ–‡ä»¶..."
for file in data/TCGA.txt data/GSE57303.txt data/GSE62254.txt.gz; do
    if [[ -f "$file" ]]; then
        echo "âœ… $file"
    else
        echo "âŒ $file ä¸å­˜åœ¨"
    fi
done

# åˆ›å»ºPythonæ–‡ä»¶ï¼ˆè¿™é‡Œéœ€è¦å°†ä¸Šé¢çš„ä»£ç å†…å®¹å†™å…¥æ–‡ä»¶ï¼‰
echo "ğŸ“ åˆ›å»ºPythonæ–‡ä»¶..."
# è¿™é‡Œéœ€è¦æ‰‹åŠ¨å¤åˆ¶ç²˜è´´ä¸Šé¢çš„ä»£ç å†…å®¹

echo "ğŸ‰ éƒ¨ç½²å®Œæˆï¼"
echo ""
echo "ğŸ“‹ ä¸‹ä¸€æ­¥ï¼š"
echo "1. cd FigureYa293AIEnhanced"
echo "2. pip install -r requirements.txt"
echo "3. ç¼–è¾‘ config.py å¡«å…¥APIå¯†é’¥"
echo "4. python main.py"
```

## ğŸš€ å¯åŠ¨å‘½ä»¤

ç°åœ¨æ‚¨å¯ä»¥è¿è¡Œï¼š

```bash
# 1. è¿›å…¥é¡¹ç›®æ ¹ç›®å½•
cd /path/to/ying-ge/FigureYa/

# 2. åˆ›å»ºé¡¹ç›®ç»“æ„
mkdir FigureYa293AIEnhanced
cd FigureYa293AIEnhanced
mkdir data results notebooks utils docs

# 3. åˆ›å»ºæ•°æ®è½¯é“¾æ¥
ln -s ../FigureYa293machineLearning/TCGA.txt data/
ln -s ../FigureYa293machineLearning/GSE57303.txt data/
ln -s ../FigureYa293machineLearning/GSE62254.txt.gz data/

# 4. åˆ›å»ºæ‰€æœ‰Pythonæ–‡ä»¶ï¼ˆå°†ä¸Šé¢çš„ä»£ç å¤åˆ¶ç²˜è´´ï¼‰

# 5. å®‰è£…ä¾èµ–
pip install -r requirements.txt

# 6. é…ç½®APIå¯†é’¥
nano config.py  # å¡«å…¥æ‚¨çš„APIå¯†é’¥

# 7. è¿è¡Œåˆ†æ
python main.py
```

æ‚¨çš„AIå¢å¼ºç‰ˆFigureYa293å°±å‡†å¤‡å¥½äº†ï¼ğŸ‰

éœ€è¦æˆ‘å¸®æ‚¨åˆ›å»ºä»»ä½•ç‰¹å®šçš„æ–‡ä»¶æˆ–è§£å†³ä»»ä½•é—®é¢˜å—ï¼Ÿ
